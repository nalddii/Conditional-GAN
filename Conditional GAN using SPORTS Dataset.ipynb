{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f36e27d-71e5-4abb-a672-0fe3139326c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision.models import efficientnet_b3\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torchvision.utils as vutils\n",
    "from models import Discriminator, Generator, initialize_weights\n",
    "from utils import gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1545d84-86e3-40c1-9f36-a70ce66a83c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_today = \"19 May 23\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "514e26f6-cea2-46d2-b1b9-f5e6d6cf3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sport_dataset = \"C:/Users/62812/Documents/Kuliah/Semester 2/Deep Learning/Tugas 4/Controlable GAN/Dataset/\"\n",
    "keterangan_data = pd.read_csv(path_sport_dataset+\"sports.csv\")\n",
    "sports_chosen = ['arm wrestling', \"basketball\", \"volleyball\", \"skydiving\", \"billiards\"]\n",
    "data_chosen = keterangan_data[keterangan_data['labels'].isin(sports_chosen)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b697bb-7b2d-48ef-be61-3c9c2aad73dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 64 #coba 64 aslinya 224\n",
    "CHANNELS_IMG = 3\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 8000\n",
    "FEATURES_CRITIC = 16\n",
    "FEATURES_GEN = 16\n",
    "CRITIC_ITERATIONS = 5\n",
    "LAMBDA_GP = 10\n",
    "NUM_CLASSES = len(sports_chosen)\n",
    "EMBED_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97986f02-2a75-4f78-8aef-b8f1b82e2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE)), #coba ganti  64, harusnya 224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "def image_to_tensor(file_path,transform):\n",
    "    img = Image.open(file_path)\n",
    "    img_tensor = transform(img)\n",
    "    return img_tensor\n",
    "\n",
    "data_label = []\n",
    "images_tensor = []\n",
    "\n",
    "\n",
    "for file,label in zip(data_chosen['filepaths'], data_chosen['labels']):\n",
    "    data_label.append(label)\n",
    "    try:\n",
    "        images_tensor.append(image_to_tensor(path_sport_dataset+file,transform))\n",
    "    except:\n",
    "        pass\n",
    "stacked_tensor = torch.stack([tensor_img for tensor_img in images_tensor], dim=0)\n",
    "\n",
    "def label_to_tensor1(labels, data_labels, IMG_SIZE):\n",
    "    assert type(labels) == list, \"Type labels harus list\"\n",
    "    label_channels = len(labels)\n",
    "    data_labels_tensors = []\n",
    "    for label in data_labels:\n",
    "        idx_encode = labels.index(label)\n",
    "        data_labels_tensors.append(idx_encode)\n",
    "    return torch.tensor(data_labels_tensors)\n",
    "\n",
    "labels_tensors = label_to_tensor1(sports_chosen,data_label,stacked_tensor.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbbc0ee9-05f6-4b6e-80d7-ea82b9a8f889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_tensors[labels_tensors==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0146869-8648-4b9f-b518-39c2990445ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arm wrestling': 109,\n",
       " 'basketball': 179,\n",
       " 'volleyball': 152,\n",
       " 'skydiving': 158,\n",
       " 'billiards': 155}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JumlahDataPerSport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d906448c-c6a4-48cb-bfd7-83da7c5a2959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset object\n",
    "dataset = TensorDataset(stacked_tensor, labels_tensors)\n",
    "\n",
    "# Create a DataLoader object with batch size and shuffle=True\n",
    "dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efdc1c7a-cb71-4dcf-9698-be3a4eb3a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eksperimen_path = f\"C:/Users/62812/Documents/Kuliah/Semester 2/Deep Learning/Tugas 4/Controlable GAN/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a397c8d-a4e9-43a9-96f7-a705ee5be538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize gen and disc, note: discriminator should be called critic,\n",
    "# according to WGAN paper (since it no longer outputs between [0, 1])\n",
    "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES,IMG_SIZE, EMBED_SIZE).to(device)\n",
    "critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC,NUM_CLASSES,IMG_SIZE).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)\n",
    "\n",
    "# initializate optimizer\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8eb3598b-2882-4ab4-b8e4-84efee92a0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (disc): Sequential(\n",
       "    (0): Conv2d(4, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (5): Conv2d(128, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "  )\n",
       "  (embed): Embedding(5, 4096)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_disc = \"C:/Users/62812/Documents/Kuliah/Semester 2/Deep Learning/Tugas 4/Controlable GAN/Discriminator Model/19 May 23_disc_sports_embed_size_5_epoch_6000.pth\"\n",
    "pt_gen = \"C:/Users/62812/Documents/Kuliah/Semester 2/Deep Learning/Tugas 4/Controlable GAN/Generator Model/19 May 23_gen_sports_embed_size_5_epoch_6000.pth\"\n",
    "critic.load_state_dict(torch.load(pt_disc))\n",
    "gen.load_state_dict(torch.load(pt_gen))\n",
    "gen.eval()\n",
    "critic.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05775e06-4568-4726-addd-a6053ea5db45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6001/8000] with  Loss D: -23.3875, loss G: 858.4497\n",
      "Epoch [6051/8000] with  Loss D: -28.4090, loss G: 858.3520\n",
      "Epoch [6101/8000] with  Loss D: -29.9366, loss G: 870.3267\n",
      "Epoch [6151/8000] with  Loss D: -23.2066, loss G: 870.5023\n",
      "Epoch [6201/8000] with  Loss D: -20.7615, loss G: 869.4408\n",
      "Epoch [6251/8000] with  Loss D: -18.4585, loss G: 865.1625\n",
      "Epoch [6301/8000] with  Loss D: -27.7621, loss G: 869.7681\n",
      "Epoch [6351/8000] with  Loss D: -29.6030, loss G: 875.7201\n",
      "Epoch [6401/8000] with  Loss D: -26.9347, loss G: 876.0898\n",
      "Epoch [6451/8000] with  Loss D: -23.5447, loss G: 875.1125\n",
      "Epoch [6501/8000] with  Loss D: -33.2492, loss G: 879.4240\n",
      "Epoch [6551/8000] with  Loss D: -32.0063, loss G: 878.6902\n",
      "Epoch [6601/8000] with  Loss D: -30.2753, loss G: 875.5422\n",
      "Epoch [6651/8000] with  Loss D: -36.8171, loss G: 890.1968\n",
      "Epoch [6701/8000] with  Loss D: -32.1020, loss G: 890.3405\n",
      "Epoch [6751/8000] with  Loss D: -32.8735, loss G: 890.0281\n",
      "Epoch [6801/8000] with  Loss D: -32.6123, loss G: 887.4672\n",
      "Epoch [6851/8000] with  Loss D: -40.7556, loss G: 899.1293\n",
      "Epoch [6901/8000] with  Loss D: -45.4540, loss G: 890.4736\n",
      "Epoch [6951/8000] with  Loss D: -24.6505, loss G: 888.9658\n",
      "Epoch [7001/8000] with  Loss D: -32.3045, loss G: 903.6573\n",
      "Epoch [7051/8000] with  Loss D: -36.8926, loss G: 892.0488\n",
      "Epoch [7101/8000] with  Loss D: -27.1285, loss G: 895.2058\n",
      "Epoch [7151/8000] with  Loss D: -28.4893, loss G: 900.5311\n",
      "Epoch [7201/8000] with  Loss D: -25.3671, loss G: 900.0892\n",
      "Epoch [7251/8000] with  Loss D: -28.0922, loss G: 900.9475\n",
      "Epoch [7301/8000] with  Loss D: -31.6941, loss G: 901.8702\n",
      "Epoch [7351/8000] with  Loss D: -27.0799, loss G: 910.3412\n",
      "Epoch [7401/8000] with  Loss D: -23.5569, loss G: 909.8708\n",
      "Epoch [7451/8000] with  Loss D: -21.8247, loss G: 901.4711\n",
      "Epoch [7501/8000] with  Loss D: -29.0109, loss G: 908.3809\n",
      "Epoch [7551/8000] with  Loss D: -30.6628, loss G: 903.8289\n",
      "Epoch [7601/8000] with  Loss D: -24.5371, loss G: 903.0345\n",
      "Epoch [7651/8000] with  Loss D: -34.0622, loss G: 910.0353\n",
      "Epoch [7701/8000] with  Loss D: -30.9335, loss G: 910.1688\n",
      "Epoch [7751/8000] with  Loss D: -22.8265, loss G: 915.8988\n",
      "Epoch [7801/8000] with  Loss D: -27.2363, loss G: 908.7537\n",
      "Epoch [7851/8000] with  Loss D: -15.8983, loss G: 908.3333\n",
      "Epoch [7901/8000] with  Loss D: -24.2423, loss G: 919.9550\n",
      "Epoch [7951/8000] with  Loss D: -26.6980, loss G: 912.6457\n",
      "Waktu running 8000 menggunakan cuda yaitu 10548.632604837418 detik\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t_start = time.time()\n",
    "# for tensorboard plotting\n",
    "fixed_noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1).to(device)\n",
    "\n",
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "for epoch in range(6000,NUM_EPOCHS):\n",
    "    for batch_idx, (real, labels) in enumerate(dataloader):\n",
    "        real = real.to(device)\n",
    "        cur_batch_size = real.shape[0]\n",
    "        labels = labels.to(device)\n",
    "        # Train Critic: max E[critic(real)] - E[critic(fake)]\n",
    "        # equivalent to minimizing the negative of that\n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
    "            fake = gen(noise,labels)\n",
    "            critic_real = critic(real,labels).reshape(-1)\n",
    "            critic_fake = critic(fake,labels).reshape(-1)\n",
    "            gp = gradient_penalty(critic,labels, real, fake, device)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
    "            )\n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step()\n",
    "\n",
    "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "        gen_fake = critic(fake,labels).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        if batch_idx == 10 and epoch % 50 == 0: \n",
    "            with torch.no_grad():\n",
    "                print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] with  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\")\n",
    "                vutils.save_image(real,f\"{eksperimen_path}/Sample Image/{date_today} - real samples epoch {epoch} batch {batch_idx}.png\", normalize = True)\n",
    "                fake = gen(noise, labels)\n",
    "                vutils.save_image(fake, f\"{eksperimen_path}/Fake Image/{date_today} - fake samples epoch {epoch} batch {batch_idx}.png\",normalize = True)\n",
    "                \n",
    "t_end = time.time()\n",
    "lama = t_end - t_start\n",
    "print(f\"Waktu running {NUM_EPOCHS} menggunakan {device} yaitu {lama} detik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e227729d-18dd-4812-b71e-68edac6d4b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "nama_dataset = \"sports\"\n",
    "discriminator_path = f\"{eksperimen_path}Discriminator Model/{date_today}_disc_{nama_dataset}_embed_size_{EMBED_SIZE}_epoch_{NUM_EPOCHS}.pth\"\n",
    "generator_path = f\"{eksperimen_path}Generator Model/{date_today}_gen_{nama_dataset}_embed_size_{EMBED_SIZE}_epoch_{NUM_EPOCHS}.pth\"\n",
    "torch.save(critic.state_dict(), discriminator_path)\n",
    "torch.save(gen.state_dict(), generator_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "597303a1-00e8-430d-9566-4bf7c0e7bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_chosen = ['arm wrestling', \"basketball\", \"volleyball\", \"skydiving\", \"billiards\"]\n",
    "def gen_gambar(model,sport_predict,sports_chosen, path, embed_size,NUM_EPOCHS):\n",
    "    with torch.no_grad():\n",
    "        jumlah_gambar = 64\n",
    "        noise = torch.randn(jumlah_gambar, Z_DIM, 1, 1).to(device)\n",
    "        idx_encode = sports_chosen.index(sport_predict)\n",
    "        encode = [idx_encode for _ in range(jumlah_gambar)]\n",
    "        labels = torch.tensor(encode).to(device)\n",
    "        pic_predict = model(noise,labels)\n",
    "        vutils.save_image(pic_predict, f\"{path} {date_today} - Hasil predict 16 gambar {sport_predict} embed size {embed_size} epoch {NUM_EPOCHS}.png\", normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dfa349fc-7fce-43e7-9aab-76ef47a96ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64, 100, 1, 1])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 100, 1, 1])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 100, 1, 1])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 100, 1, 1])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 100, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for sport in sports_chosen:\n",
    "    path_test = \"C:/Users/62812/Documents/Kuliah/Semester 2/Deep Learning/Tugas 4/Controlable GAN/Test Image/\"\n",
    "    gen_gambar(gen,sport, sports_chosen, path_test, EMBED_SIZE, NUM_EPOCHS)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0c018-1a3c-4b31-ae18-3dd8b9cf9900",
   "metadata": {},
   "source": [
    "Waktu running 300 epoch pertama : 1199.4615950584412 \n",
    "waktu running 300 epoch kedua : 1112.2095129489899 \n",
    "kedua running 4000 epoch ketiga : 13179.149538993835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52eff189-e599-4137-a3fd-2351b3695568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.660833333333333"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa9d94-0dff-4ece-9a96-04341244c692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
