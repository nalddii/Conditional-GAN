{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9e06dc3-961f-4199-a6c9-65dafc79a6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data1 = np.array([100,32,400,500,150,40])\n",
    "data2 = np.array([32,29,65,100,123]) \n",
    "th = 50\n",
    "\n",
    "def filter_irisan(data1,data2,th):\n",
    "    filter_data1 = data1[data1>th]\n",
    "    filter_data2 = data2[data2>th]\n",
    "    output = []\n",
    "    for i in filter_data1:\n",
    "        if i in filter_data2:\n",
    "            output.append(i)\n",
    "    return output\n",
    "\n",
    "filter_irisan(data1,data2,th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "816ce5c7-9c9e-463e-8453-4c12557f93cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision.models import efficientnet_b3\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torchvision.utils as vutils\n",
    "from models import Discriminator, Generator, initialize_weights\n",
    "from utils import gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d26af056-4a9b-4063-b3fc-f1ac332f730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_today = \"24 May 23\"\n",
    "jenis_dataset = \"dog_anime_celeb\"\n",
    "nama_dataset = [\"anime\", \"celeb\", \"dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0daa9736-259d-4c2c-8cf8-2179126ee41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cgan2 = \"C:/Users/62812/Documents/Kuliah/Semester 2/Deep Learning/Tugas 4/Controlable GAN2/Dataset 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae49119a-2a52-4c27-814a-965adb9d88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = 64 #coba 64 aslinya 224\n",
    "CHANNELS_IMG = 3\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 1000\n",
    "FEATURES_CRITIC = 16\n",
    "FEATURES_GEN = 16\n",
    "CRITIC_ITERATIONS = 5\n",
    "LAMBDA_GP = 10\n",
    "NUM_CLASSES = len(nama_dataset)\n",
    "EMBED_SIZE = len(nama_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22735309-7cbe-4543-8ce0-1915fc3e93c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE)), #coba ganti  64, harusnya 224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "def create_dataset(path,nama_dataset,transform):\n",
    "    files = os.listdir(path)\n",
    "    images = []\n",
    "    labels = []\n",
    "    for file in files:\n",
    "        label = nama_dataset.index(file)\n",
    "        file_images = os.listdir(f\"{path}/{file}\")\n",
    "        for img in file_images:\n",
    "            img_i = Image.open(f\"{path}/{file}/{img}\")\n",
    "            images.append(transform(img_i))\n",
    "            labels.append(label)\n",
    "    stacked_tensor = torch.stack([tensor_img for tensor_img in images], dim=0)\n",
    "    return stacked_tensor,torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1c499b5-5fb4-472a-8658-036078f2483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X , label = create_dataset(path_cgan2, nama_dataset, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed95d6b6-0c61-4a9a-b96a-e3ff6102ecb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anime': 1000, 'celeb': 1000, 'dog': 821}\n"
     ]
    }
   ],
   "source": [
    "jumlah_per_jenis_gambar = {}\n",
    "for n,jenis_gambar in enumerate(nama_dataset):\n",
    "    jumlah_per_jenis_gambar[jenis_gambar] = len(label[label==n])\n",
    "print(jumlah_per_jenis_gambar)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fb8a32d-38ca-469e-a701-08eaaa22ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset object\n",
    "dataset = TensorDataset(X, label)\n",
    "\n",
    "# Create a DataLoader object with batch size and shuffle=True\n",
    "dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5953383-f064-47d5-9d57-b8f37e111d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize gen and disc, note: discriminator should be called critic,\n",
    "# according to WGAN paper (since it no longer outputs between [0, 1])\n",
    "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES,IMG_SIZE, EMBED_SIZE).to(device)\n",
    "critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC,NUM_CLASSES,IMG_SIZE).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)\n",
    "\n",
    "# initializate optimizer\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f25a9c7-ffc3-4452-91cf-5cf38978af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "path_eksperimen = \"C:/Users/62812/Documents/Kuliah/Semester 2/Deep Learning/Tugas 4/Controlable GAN2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b097d42-9ec9-4237-8f8b-5a7e34a0744d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# path_eksperimen = \"C:/Users/62812/Documents/Kuliah/Semester 2/Deep Learning/Tugas 4/Controlable GAN2\"\n",
    "# t_start = time.time()\n",
    "# # for tensorboard plotting\n",
    "# fixed_noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1).to(device)\n",
    "\n",
    "# gen.train()\n",
    "# critic.train()\n",
    "\n",
    "# for epoch in range(100,NUM_EPOCHS):\n",
    "#     for batch_idx, (real, labels) in enumerate(dataloader):\n",
    "#         real = real.to(device)\n",
    "#         cur_batch_size = real.shape[0]\n",
    "#         labels = labels.to(device)\n",
    "#         # Train Critic: max E[critic(real)] - E[critic(fake)]\n",
    "#         # equivalent to minimizing the negative of that\n",
    "#         for _ in range(CRITIC_ITERATIONS):\n",
    "#             noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
    "#             fake = gen(noise,labels)\n",
    "#             critic_real = critic(real,labels).reshape(-1)\n",
    "#             critic_fake = critic(fake,labels).reshape(-1)\n",
    "#             gp = gradient_penalty(critic,labels, real, fake, device)\n",
    "#             loss_critic = (\n",
    "#                 -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
    "#             )\n",
    "#             critic.zero_grad()\n",
    "#             loss_critic.backward(retain_graph=True)\n",
    "#             opt_critic.step()\n",
    "\n",
    "#         # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "#         gen_fake = critic(fake,labels).reshape(-1)\n",
    "#         loss_gen = -torch.mean(gen_fake)\n",
    "#         gen.zero_grad()\n",
    "#         loss_gen.backward()\n",
    "#         opt_gen.step()\n",
    "#         if batch_idx == 42 and epoch % 10 == 0: \n",
    "#             with torch.no_grad():\n",
    "#                 print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] with  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\")\n",
    "#                 vutils.save_image(real,f\"{path_eksperimen}/Sample Image/{date_today} - real samples epoch {epoch} batch {batch_idx}.png\", normalize = True)\n",
    "#                 fake = gen(noise, labels)\n",
    "#                 vutils.save_image(fake, f\"{path_eksperimen}/Fake Image/{date_today} - fake samples epoch {epoch} batch {batch_idx}.png\",normalize = True)\n",
    "                \n",
    "# t_end = time.time()\n",
    "# lama = t_end - t_start\n",
    "# print(f\"Waktu running {NUM_EPOCHS} menggunakan {device} yaitu {lama} detik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cea0d018-7691-4042-96c7-1a1d3eeddf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "discriminator_path = f\"{path_eksperimen}/Discriminator Model/{date_today}_disc_{jenis_dataset}_embed_size_{EMBED_SIZE}_epoch_{NUM_EPOCHS}.pth\"\n",
    "generator_path = f\"{path_eksperimen}/Generator Model/{date_today}_gen_{jenis_dataset}_embed_size_{EMBED_SIZE}_epoch_{NUM_EPOCHS}.pth\"\n",
    "torch.save(critic.state_dict(), discriminator_path)\n",
    "torch.save(gen.state_dict(), generator_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19a0bef1-83c2-46f7-95ee-4f0708dc5b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_gambar(model,class_predict,jumlah_gambar, nama_dataset, path, embed_size,NUM_EPOCHS):\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(jumlah_gambar, Z_DIM, 1, 1).to(device)\n",
    "        idx_encode = nama_dataset.index(class_predict)\n",
    "        encode = [idx_encode for _ in range(jumlah_gambar)]\n",
    "        labels = torch.tensor(encode).to(device)\n",
    "        pic_predict = model(noise,labels)\n",
    "        vutils.save_image(pic_predict, f\"{path} {date_today} - Hasil predict gambar {class_predict} embed size {embed_size} epoch {NUM_EPOCHS}.png\", normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6f70391-d71a-4225-b983-e378b2a6c29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (net): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(103, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): Tanh()\n",
       "  )\n",
       "  (embed): Embedding(3, 3)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_model_generator = \"C:/Users/62812/Documents/Kuliah/Semester 2/Deep Learning/Tugas 4/Controlable GAN2/Generator Model/19 May 23_gen_dog_anime_celeb_embed_size_3_epoch_1000.pth\"\n",
    "gen.load_state_dict(torch.load(path_model_generator))\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fce05a8-129d-457b-8f33-3833a0c456a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randn() received an invalid combination of arguments - got (list, int, int, int), but expected one of:\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m path_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/62812/Documents/Kuliah/Semester 2/Deep Learning/Tugas 4/Controlable GAN2/Test Image/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mgen_gambar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAnime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnama_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEMBED_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m, in \u001b[0;36mgen_gambar\u001b[1;34m(model, class_predict, jumlah_gambar, nama_dataset, path, embed_size, NUM_EPOCHS)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen_gambar\u001b[39m(model,class_predict,jumlah_gambar, nama_dataset, path, embed_size,NUM_EPOCHS):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 3\u001b[0m         noise \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjumlah_gambar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ_DIM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m         idx_encode \u001b[38;5;241m=\u001b[39m nama_dataset\u001b[38;5;241m.\u001b[39mindex(class_predict)\n\u001b[0;32m      5\u001b[0m         encode \u001b[38;5;241m=\u001b[39m [idx_encode \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(jumlah_gambar)]\n",
      "\u001b[1;31mTypeError\u001b[0m: randn() received an invalid combination of arguments - got (list, int, int, int), but expected one of:\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "path_test = \"C:/Users/62812/Documents/Kuliah/Semester 2/Deep Learning/Tugas 4/Controlable GAN2/Test Image/\"\n",
    "gen_gambar(gen,\"Anime\", nama_dataset ,64, path_test, EMBED_SIZE, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b965a87-96d4-44fb-bb08-455c86e5d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_predict in nama_dataset:\n",
    "    path_test = \"C:/Users/62812/Documents/Kuliah/Semester 2/Deep Learning/Tugas 4/Controlable GAN2/Test Image/\"\n",
    "    gen_gambar(gen,class_predict, nama_dataset, path_test, EMBED_SIZE, NUM_EPOCHS)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63979d3f-da9b-4824-ae18-ca8c20c7d50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6487212707001282"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.exp(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5fa418-3020-45a7-bbff-458f4998047f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3d72b-b509-48c7-b846-dd25c7dcfbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
